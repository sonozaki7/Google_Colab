{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonozaki7/Google_Colab/blob/main/Stable_Diffusion_(Mike!).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: There might be a handful of bugs at the moment. The developers of this stable diffusion implementation keep changing the api. Everyone should know not to make breaking api changes so regularly! I'll do a pass over the code and fix bugs as soon as I can. Am away this week :) thanks to Michael d for bringing this to my attention. "
      ],
      "metadata": {
        "id": "6FCMR8tDQRf6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJWFTB8aVxy-"
      },
      "source": [
        "# Stable Diffusion Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cbmk3JqVs5yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333583ee-b33c-4471-a065-80e49e4397e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.16.1-py3-none-any.whl (934 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.9/934.9 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (6.6.0)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.15.1+cu118)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.10.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (3.25.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, diffusers, lpips, accelerate\n",
            "Successfully installed accelerate-0.18.0 diffusers-0.16.1 huggingface-hub-0.14.1 lpips-0.1.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers diffusers lpips accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nl3JP8iYUTVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "3d39e352e80d417e92a3c3e412ecdd24",
            "29066e44578f4f338354077f60a40116",
            "b1e024494dae46ae84d6b35b7eebc3de",
            "af74310dfbcb40fc9261125df38b6f5a",
            "4021b5bf5b4347dab0e7ad7802744436",
            "932024b6ec20477d92ac5f3add9c3a30",
            "42793cae001e43d2ae3a2363709282ba",
            "0f614a671a754e02af0da9e2b6a3d55c",
            "eeb243cafc4c4957bd97bb9c9567f240",
            "4caed84228c846af813c371bad7ee280",
            "5cb97622c3354fa096efc50a77905077",
            "0aa445fd5ded4230be085568a7b6a163",
            "1e99dc3a47464bf68b59484fc7e422dd",
            "7b87441ec0ff477ea3a51e253df8390c",
            "77d5f8e3ca504c4aad23229779c3c548",
            "51cfb817d5804eedadb084f5da0b1b5f",
            "2ed6b02e1daa45e5a1fdefc3e05b9165"
          ]
        },
        "outputId": "fa71d01c-4db6-4a93-c882-492685ca2450"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d39e352e80d417e92a3c3e412ecdd24"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RtV9OsG2UkUD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "from torchvision import transforms as tfms\n",
        "\n",
        "# For video display:\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Set device\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t96p567IUqzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed6181d7-c5bb-4c1b-ddd4-d16aa9cae6cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m356\u001b[0m in \u001b[92mload_config\u001b[0m      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m353 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m354 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m355 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Load from URL or cache if already cached\u001b[0m                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m356 \u001b[2m│   │   │   │   \u001b[0mconfig_file = hf_hub_download(                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m357 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpretrained_model_name_or_path,                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m358 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mfilename=\u001b[96mcls\u001b[0m.config_name,                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m359 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcache_dir=cache_dir,                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m120\u001b[0m in \u001b[92m_inner_fn\u001b[0m    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__name__\u001b[0m, has_token=ha   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m120 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m1181\u001b[0m in \u001b[92mhf_hub_download\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1178 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1179 \u001b[0m\u001b[2m│   \u001b[0murl = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1180 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1181 \u001b[2m│   \u001b[0mheaders = build_hf_headers(                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1182 \u001b[0m\u001b[2m│   │   \u001b[0mtoken=token,                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1183 \u001b[0m\u001b[2m│   │   \u001b[0mlibrary_name=library_name,                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1184 \u001b[0m\u001b[2m│   │   \u001b[0mlibrary_version=library_version,                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m120\u001b[0m in \u001b[92m_inner_fn\u001b[0m    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__name__\u001b[0m, has_token=ha   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m120 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_headers.py\u001b[0m:\u001b[94m117\u001b[0m in                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mbuild_hf_headers\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mIf `token=True` but token is not saved locally.\u001b[0m                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Get auth token to send\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m117 \u001b[2m│   \u001b[0mtoken_to_send = get_token_to_send(token)                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   \u001b[0m_validate_token_to_send(token_to_send, is_write_action=is_write_action)                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Combine headers\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_headers.py\u001b[0m:\u001b[94m149\u001b[0m in                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mget_token_to_send\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Case token is explicitly required\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m token \u001b[95mis\u001b[0m \u001b[94mTrue\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m cached_token \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m149 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mEnvironmentError\u001b[0m(                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mToken is required (`token=True`), but no token found. You\u001b[0m\u001b[33m\"\u001b[0m                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m need to provide a token or be logged in to Hugging Face with\u001b[0m\u001b[33m\"\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[0m\u001b[33m\"\u001b[0m                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mOSError: \u001b[0mToken is required \u001b[1m(\u001b[0m`\u001b[33mtoken\u001b[0m=\u001b[3;92mTrue\u001b[0m`\u001b[1m)\u001b[0m, but no token found. You need to provide a token or be logged in to \n",
              "Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See \u001b[4;94mhttps://huggingface.co/settings/tokens.\u001b[0m\n",
              "\n",
              "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
              "\n",
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m494\u001b[0m in                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m491 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m492 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m493 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# load config\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m494 \u001b[2m│   │   \u001b[0mconfig, unused_kwargs, commit_hash = \u001b[96mcls\u001b[0m.load_config(                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m495 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_path,                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m496 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m497 \u001b[0m\u001b[2m│   │   │   \u001b[0mreturn_unused_kwargs=\u001b[94mTrue\u001b[0m,                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m400\u001b[0m in \u001b[92mload_config\u001b[0m      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m\u001b[33mhttps://huggingface.co/docs/diffusers/installation#offline-mode\u001b[0m\u001b[33m'\u001b[0m\u001b[33m.\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m398 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m399 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mEnvironmentError\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m400 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mEnvironmentError\u001b[0m(                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCan\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt load config for \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mpretrained_model_name_or_path\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m. If you we\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m\u001b[33mhttps://huggingface.co/models\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, make sure you don\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt have a local d\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m403 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mOtherwise, make sure \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mpretrained_model_name_or_path\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m is the corr\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mOSError: \u001b[0mCan't load config for \u001b[32m'CompVis/stable-diffusion-v1-4'\u001b[0m. If you were trying to load it from \n",
              "\u001b[32m'https://huggingface.co/models'\u001b[0m, make sure you don't have a local directory with the same name. Otherwise, make \n",
              "sure \u001b[32m'CompVis/stable-diffusion-v1-4'\u001b[0m is the correct path to a directory containing a config.json file\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/diffusers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">356</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_config</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">353 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">354 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">355 │   │   │   │   # Load from URL or cache if already cached</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>356 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>config_file = hf_hub_download(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">357 │   │   │   │   │   </span>pretrained_model_name_or_path,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">358 │   │   │   │   │   </span>filename=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.config_name,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">359 │   │   │   │   │   </span>cache_dir=cache_dir,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">120</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_fn</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> check_use_auth_token:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   │   │   </span>kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, has_token=ha   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>120 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _inner_fn  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">file_download.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1181</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">hf_hub_download</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1178 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1179 │   </span>url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1180 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1181 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>headers = build_hf_headers(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1182 │   │   </span>token=token,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1183 │   │   </span>library_name=library_name,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1184 │   │   </span>library_version=library_version,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">120</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_fn</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> check_use_auth_token:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   │   │   </span>kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, has_token=ha   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>120 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _inner_fn  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_headers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">117</span> in                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">build_hf_headers</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">If `token=True` but token is not saved locally.</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   # Get auth token to send</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>117 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>token_to_send = get_token_to_send(token)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   </span>_validate_token_to_send(token_to_send, is_write_action=is_write_action)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   # Combine headers</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_headers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">149</span> in                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_token_to_send</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   # Case token is explicitly required</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> cached_token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>149 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">EnvironmentError</span>(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Token is required (`token=True`), but no token found. You\"</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" need to provide a token or be logged in to Hugging Face with\"</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" `huggingface-cli login` or `huggingface_hub.login`. See\"</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OSError: </span>Token is required <span style=\"font-weight: bold\">(</span>`<span style=\"color: #808000; text-decoration-color: #808000\">token</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>`<span style=\"font-weight: bold\">)</span>, but no token found. You need to provide a token or be logged in to \n",
              "Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/settings/tokens.</span>\n",
              "\n",
              "<span style=\"font-style: italic\">During handling of the above exception, another exception occurred:</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">494</span> in                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">491 │   │   </span>}                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">492 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">493 │   │   # load config</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>494 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>config, unused_kwargs, commit_hash = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.load_config(                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">495 │   │   │   </span>config_path,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">496 │   │   │   </span>cache_dir=cache_dir,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">497 │   │   │   </span>return_unused_kwargs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/diffusers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">400</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_config</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">397 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" 'https://huggingface.co/docs/diffusers/installation#offline-mode'.</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">398 │   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">399 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">EnvironmentError</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>400 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">EnvironmentError</span>(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">401 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Can't load config for '{</span>pretrained_model_name_or_path<span style=\"color: #808000; text-decoration-color: #808000\">}'. If you we</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">402 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"'https://huggingface.co/models', make sure you don't have a local d</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">403 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Otherwise, make sure '{</span>pretrained_model_name_or_path<span style=\"color: #808000; text-decoration-color: #808000\">}' is the corr</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OSError: </span>Can't load config for <span style=\"color: #008000; text-decoration-color: #008000\">'CompVis/stable-diffusion-v1-4'</span>. If you were trying to load it from \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/models'</span>, make sure you don't have a local directory with the same name. Otherwise, make \n",
              "sure <span style=\"color: #008000; text-decoration-color: #008000\">'CompVis/stable-diffusion-v1-4'</span> is the correct path to a directory containing a config.json file\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the autoencoder model which will be used to decode the latents into image space. \n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "# The noise scheduler\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "# To the GPU we go!\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQM7XgxYV6hg"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYoviVgUXjwZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"A digital illustration of a steampunk computer laboratory with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors\"]\n",
        "height = 512\n",
        "width = 768\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 7.5\n",
        "generator = torch.manual_seed(4)\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "(batch_size, unet.in_channels, height // 8, width // 8),\n",
        "generator=generator,\n",
        ")\n",
        "\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "    # Loop\n",
        "with autocast(\"cuda\"):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents)\n",
        "\n",
        "# Display\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "ntjZ-QbCxVSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Loop"
      ],
      "metadata": {
        "id": "KMcYtRltEmzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etHajiDAUyMO"
      },
      "outputs": [],
      "source": [
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/electric\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/electric\n",
        "\n",
        "# Some settings\n",
        "#prompt = [\"a futuristic city, abandoned and overgrown with plants, dystopia, bathed in sunlight\"]\n",
        "prompt = [\"an electric sky city on an alien world\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 768                         # default width of Stable Diffusion\n",
        "num_inference_steps = 50            # Number of denoising steps\n",
        "guidance_scale = 8.0                # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(1200)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "for idx in range(100):\n",
        "    generator = torch.manual_seed(idx)\n",
        "\n",
        "    # Prep latents\n",
        "    latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        "    )\n",
        "    latents = latents.to(torch_device)\n",
        "    latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "    # Loop\n",
        "    with autocast(\"cuda\"):\n",
        "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            sigma = scheduler.sigmas[i]\n",
        "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # predict the noise residual\n",
        "            with torch.no_grad():\n",
        "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "    # scale and decode the image latents with vae\n",
        "    latents = 1 / 0.18215 * latents\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents)\n",
        "    \n",
        "    # Display\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    pil_images[0].save(f'/content/drive/MyDrive/stablediff/electric/{idx:04}.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITl1oLuxWDGf"
      },
      "source": [
        "# Scheduling and Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU-g0cqfWHGp"
      },
      "outputs": [],
      "source": [
        "# Using torchvision.transforms.ToTensor\n",
        "to_tensor_tfm = tfms.ToTensor()\n",
        "\n",
        "def pil_to_latent(input_im):\n",
        "  # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
        "  with torch.no_grad():\n",
        "    latent = vae.encode(to_tensor_tfm(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
        "  return 0.18215 * latent.mode() # or .mean or .sample\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "  # bath of latents -> list of images\n",
        "  latents = (1 / 0.18215) * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents)[0]\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  return pil_images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open('/content/drive/MyDrive/stablediff/shitake3.png').convert('RGB')\n",
        "im = im.resize((512,512))\n",
        "encoded = pil_to_latent(im)\n",
        "im"
      ],
      "metadata": {
        "id": "0Lb7v2EfICDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image to Image"
      ],
      "metadata": {
        "id": "_ve_2EROK1Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpjEKYlXXFd0"
      },
      "outputs": [],
      "source": [
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/noise2\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/noise2\n",
        "\n",
        "# View a noised version\n",
        "noise = torch.randn_like(encoded) # Random noise\n",
        "\n",
        "for i in tqdm(range(200)):\n",
        "    timestep = i * 5\n",
        "    encoded_and_noised = scheduler.add_noise(encoded, noise, timestep)\n",
        "    img = latents_to_pil(encoded_and_noised)[0]\n",
        "    img.save(f'/content/drive/MyDrive/stablediff/noise/{i:04}.jpeg')\n",
        "\n",
        "!ffmpeg -v 1 -y -f image2 -framerate 12 -i /content/drive/MyDrive/stablediff/noise/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n",
        "mp4 = open('out.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=512 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCIn_9XnXzAr"
      },
      "outputs": [],
      "source": [
        "prompt = [\"wooden carving of a rabbit eating a leaf, highly detailed, 4k, artisan\"]\n",
        "height = 512                       \n",
        "width = 512                        \n",
        "num_inference_steps = 50   \n",
        "guidance_scale = 8            \n",
        "generator = torch.manual_seed(1)   \n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Start step\n",
        "start_step = 25\n",
        "start_sigma = scheduler.sigmas[start_step]\n",
        "start_timestep = int(scheduler.timesteps[start_step])\n",
        "\n",
        "# Prep latents\n",
        "noise = torch.randn_like(encoded)\n",
        "latents = scheduler.add_noise(encoded, noise, start_timestep)\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * start_sigma  # << NB\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    if i > start_step:\n",
        "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "      sigma = scheduler.sigmas[i]\n",
        "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "      # predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "      # perform guidance\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "latents_to_pil(latents)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed guidance"
      ],
      "metadata": {
        "id": "VYwsF4udLXY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB3o7-WZGo5l"
      },
      "outputs": [],
      "source": [
        "prompts = ['blue fire', 'reticulated python in a tree']\n",
        "weights = [0.5,0.5]\n",
        "height = 512                \n",
        "width = 768                      \n",
        "num_inference_steps = 50 \n",
        "guidance_scale = 8 \n",
        "generator = torch.manual_seed(5)\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text \n",
        "# Embed both prompts\n",
        "text_embeddings = []\n",
        "for i in range(len(prompts)):\n",
        "    text_input = tokenizer([prompts[i]], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        text_embeddings.append(text_encoder(text_input.input_ids.to(torch_device))[0])\n",
        "\n",
        "# Take the average\n",
        "weighted_embeddings = torch.zeros(text_embeddings[0].shape).to(torch_device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(prompts)):\n",
        "        weighted_embeddings.add_(text_embeddings[i] * weights[i])\n",
        "\n",
        "text_embeddings = weighted_embeddings\n",
        "\n",
        "# And the uncond. input as before:\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    sigma = scheduler.sigmas[i]\n",
        "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "\n",
        "latents_to_pil(latents)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising Example"
      ],
      "metadata": {
        "id": "QV4JlBIZqxvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By-8CPHZM_v8"
      },
      "outputs": [],
      "source": [
        "prompt = 'a futuristic city, abandoned and overgrown with plants, dystopia, bathed in sunlight'\n",
        "height = 512                     \n",
        "width = 768 \n",
        "num_inference_steps = 100 \n",
        "guidance_scale = 8     \n",
        "generator = torch.manual_seed(10) \n",
        "batch_size = 1\n",
        "\n",
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/denoising2/\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/denoising2/\n",
        "\n",
        "# Prep text \n",
        "text_input = tokenizer([prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "# And the uncond. input as before:\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    \n",
        "    im_input = latents_to_pil(latents)[0]\n",
        "\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    sigma = scheduler.sigmas[i]\n",
        "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "    \n",
        "    # Get the predicted x0:\n",
        "    latents_x0 = latents - sigma * noise_pred\n",
        "\n",
        "    im_t0 = latents_to_pil(latents_x0)[0]\n",
        "    im_noise = latents_to_pil(sigma * noise_pred)[0]\n",
        "\n",
        "    # And the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "    im_next = latents_to_pil(latents)[0]\n",
        "\n",
        "    # Combine the two images and save for later viewing\n",
        "    im = Image.new('RGB', (2304, 512))\n",
        "    im.paste(im_input, (0, 0))\n",
        "    im.paste(im_noise, (768, 0))\n",
        "    im.paste(im_t0, (1536, 0))\n",
        "    im.save(f'/content/drive/MyDrive/stablediff/denoising2/{i:04}.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg41ncctOnpC"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -v 1 -y -f image2 -framerate 12 -i /content/drive/MyDrive/stablediff/denoising2/%04d.jpg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n",
        "mp4 = open('out.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=1536 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d39e352e80d417e92a3c3e412ecdd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29066e44578f4f338354077f60a40116",
              "IPY_MODEL_b1e024494dae46ae84d6b35b7eebc3de",
              "IPY_MODEL_af74310dfbcb40fc9261125df38b6f5a",
              "IPY_MODEL_4021b5bf5b4347dab0e7ad7802744436",
              "IPY_MODEL_932024b6ec20477d92ac5f3add9c3a30"
            ],
            "layout": "IPY_MODEL_42793cae001e43d2ae3a2363709282ba"
          }
        },
        "29066e44578f4f338354077f60a40116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f614a671a754e02af0da9e2b6a3d55c",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb243cafc4c4957bd97bb9c9567f240",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b1e024494dae46ae84d6b35b7eebc3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4caed84228c846af813c371bad7ee280",
            "placeholder": "​",
            "style": "IPY_MODEL_5cb97622c3354fa096efc50a77905077",
            "value": ""
          }
        },
        "af74310dfbcb40fc9261125df38b6f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0aa445fd5ded4230be085568a7b6a163",
            "style": "IPY_MODEL_1e99dc3a47464bf68b59484fc7e422dd",
            "value": true
          }
        },
        "4021b5bf5b4347dab0e7ad7802744436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7b87441ec0ff477ea3a51e253df8390c",
            "style": "IPY_MODEL_77d5f8e3ca504c4aad23229779c3c548",
            "tooltip": ""
          }
        },
        "932024b6ec20477d92ac5f3add9c3a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51cfb817d5804eedadb084f5da0b1b5f",
            "placeholder": "​",
            "style": "IPY_MODEL_2ed6b02e1daa45e5a1fdefc3e05b9165",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "42793cae001e43d2ae3a2363709282ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0f614a671a754e02af0da9e2b6a3d55c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb243cafc4c4957bd97bb9c9567f240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4caed84228c846af813c371bad7ee280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cb97622c3354fa096efc50a77905077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aa445fd5ded4230be085568a7b6a163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e99dc3a47464bf68b59484fc7e422dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b87441ec0ff477ea3a51e253df8390c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d5f8e3ca504c4aad23229779c3c548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "51cfb817d5804eedadb084f5da0b1b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ed6b02e1daa45e5a1fdefc3e05b9165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}